!pip install protobuf==3.20
!pip install deepface
from google.colab import drive
drive.mount('/content/drive')

# Set the data set path
input_dir = '/content/drive/My Drive/CPS4982dataset'
output_dir = '/content/drive/My Drive/CPS4982dataset_sorted'

# Create a sorted folder
import os
emotion_classes = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
os.makedirs(output_dir, exist_ok=True)
for emotion in emotion_classes:
    os.makedirs(os.path.join(output_dir, emotion), exist_ok=True)

from deepface import DeepFace
import shutil
import os
from PIL import Image

# Configure Paths
input_dir = '/content/drive/My Drive/CPS4982dataset'
output_dir = '/content/drive/My Drive/CPS4982dataset_sorted'

# Create a sorted folder
emotion_classes = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
os.makedirs(output_dir, exist_ok=True)
for emotion in emotion_classes:
    os.makedirs(os.path.join(output_dir, emotion), exist_ok=True)

# Traverse and classify the images
image_files = [f for f in os.listdir(input_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
for image_file in image_files:
    image_path = os.path.join(input_dir, image_file)
    try:
        # Use DeepFace for sentiment analysis
        analysis = DeepFace.analyze(img_path=image_path, actions=['emotion'], enforce_detection=False)

        # Extract 'dominant_emotion'
        emotions = analysis[0]['emotion']  # Retrieves the 'emotion' dictionary for the first element from the returned list
        predicted_emotion = max(emotions, key=emotions.get)  # Get the emotion category with the greatest probability value

        #Move the picture to the emotion category folder
        if predicted_emotion in emotion_classes:
            target_folder = os.path.join(output_dir, predicted_emotion)
            shutil.move(image_path, os.path.join(target_folder, image_file))
            print(f"Image {image_file} Classified as {predicted_emotion}")
        else:
            print(f"Image {image_file} classification failed, emotion not recognized")
    except Exception as e:
        print(f"Error processing image {image_file} : {e}")
               
for emotion in emotion_classes:
    folder_path = os.path.join(output_dir, emotion)
    num_images = len(os.listdir(folder_path))
    print(f"Category: {emotion}, Number of images: {num_images}")

import matplotlib.pyplot as plt

# Count pictures
categories = []
counts = []
for emotion in emotion_classes:
    folder_path = os.path.join(output_dir, emotion)
    num_images = len(os.listdir(folder_path))
    categories.append(emotion)
    counts.append(num_images)

# Draw a bar chart
plt.bar(categories, counts)
plt.xlabel('Emotion Categories')
plt.ylabel('Number of Images')
plt.title('Emotion Classification Distribution')
plt.xticks(rotation=45)
plt.show()

import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix

# Define the true number of labels and the predicted number
true_counts = [4125, 436, 4147, 7245, 4830, 3171, 4985]
predicted_counts = [3716, 279, 3874, 7233, 4868, 2939, 6030]

# Calculate the correct number of categories
correct_counts = [min(true, pred) for true, pred in zip(true_counts, predicted_counts)]

# Construct confusion matrix
conf_matrix = np.zeros((7, 7))
np.fill_diagonal(conf_matrix, correct_counts)

# Assume a uniform distribution of classification errors
for i in range(len(conf_matrix)):
    true_error = true_counts[i] - correct_counts[i]
    pred_error = predicted_counts[i] - correct_counts[i]
    error_distributed = min(true_error, pred_error) / (len(conf_matrix) - 1)
    conf_matrix[i] += error_distributed

# Print confusion matrix
labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
print(pd.DataFrame(conf_matrix, columns=labels, index=labels))

# Computational accuracy
total_correct = sum(correct_counts)
total_images = sum(true_counts)
accuracy = total_correct / total_images
print(f"Accuracy: {accuracy:.2%}")

from sklearn.metrics import precision_score

# Constructing Real Tags and predictive Tags (simplified version)
y_true = ['Angry'] * true_counts[0] + ['Disgust'] * true_counts[1] + \
         ['Fear'] * true_counts[2] + ['Happy'] * true_counts[3] + \
         ['Sad'] * true_counts[4] + ['Surprise'] * true_counts[5] + ['Neutral'] * true_counts[6]

y_pred = ['Angry'] * predicted_counts[0] + ['Disgust'] * predicted_counts[1] + \
         ['Fear'] * predicted_counts[2] + ['Happy'] * predicted_counts[3] + \
         ['Sad'] * predicted_counts[4] + ['Surprise'] * predicted_counts[5] + ['Neutral'] * predicted_counts[6]

# Computational accuracy
precision = precision_score(y_true, y_pred, average='macro')
print(f"Precision: {precision:.2%}")

from sklearn.metrics import recall_score

# Calculate Recall
recall = recall_score(y_true, y_pred, average='macro')
print(f"Recall: {recall:.2%}")

from sklearn.metrics import f1_score

# Calculate F1- Score
f1 = f1_score(y_true, y_pred, average='macro')
print(f"F1-Score: {f1:.2%}")


